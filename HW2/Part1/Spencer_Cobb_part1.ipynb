{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52aac0c151b3e181",
   "metadata": {},
   "source": [
    "# HW2 part 2: GNN \n",
    "\n",
    "## Graph Attention Network (GAT) Implementation for Node Classification\n",
    "\n",
    "**Objective:** Implement a GAT from scratch to perform node classification using an OGB dataset. Develop neural components, including the forward pass, as well as training and testing routines. There are 17 `todo`s and 2 questions for GAT.\n",
    "\n",
    "A Graph Attention Network (GAT) applies the concept of self-attention to graph-structured data. Unlike traditional Graph Convolutional Networks (GCNs) that rely on fixed or uniform weights derived from adjacency structures, GAT learns to assign different weights (attention scores) to different edges. This allows the model to focus more on important neighbors while possibly ignoring less relevant ones. By doing so, GAT effectively captures how each node interacts with its neighbors in a more flexible and adaptive way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dfc94c7bb788a68",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-10T19:47:13.575637Z",
     "start_time": "2025-03-10T19:47:06.726046Z"
    }
   },
   "outputs": [],
   "source": [
    "# This notebook's first part demonstrates how to train and evaluate a GAT model on the ogbn-arxiv dataset for node classification (To predict the category of each paper). Make sure you know the basics of GNNs, PyG and pytorch before starting this notebook. If not, please check the corresponding tutorials first.\n",
    "\n",
    "# If you use Google Colab, you can uncomment and run the following command to install the required packages.\n",
    "# For this assigment, we recommend using your own local computer since the RAM usage is high, Colab may crash due to the high RAM usage.\n",
    "# We passed our assignment on our local computers using python version 3.10.16.\n",
    "# !pip install torch==2.5.0\n",
    "# !pip install torch-geometric==2.6.1\n",
    "# !pip install ogb==1.3.6\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch_geometric.nn import GATConv\n",
    "from ogb.nodeproppred import PygNodePropPredDataset,Evaluator\n",
    "\n",
    "# We recommend to use GPU for training if possible. Because GAT is a relatively large model, training on CPU can be slow. But after testing, we find that training GAT on CPU is also acceptable.\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")  # NVIDIA GPU\n",
    "else:\n",
    "    device = torch.device(\"cpu\")   # CPU fallback\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "\n",
    "import random\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    # make cudnn deterministic\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# set a random seed for reproducibility\n",
    "set_seed(42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d19bf43fc6e61d5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-10T19:47:13.660163Z",
     "start_time": "2025-03-10T19:47:13.586988Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# ## Define a GAT Model for Node Classification\n",
    "\n",
    "#### Model Structure\n",
    "\n",
    "## The model is divided into two GATConv layers:\n",
    "##   The first layer uses multi-head attention (specified by num_heads) to produce richer representations. Here, each head outputs hidden_channels features, and they are concatenated, resulting in a total output dimension of hidden_channels * num_heads.\n",
    "##   The second layer is a single-head output layer that directly predicts the final node embeddings or categories. Its output dimension corresponds to the number of classes (out_channels).\n",
    "\n",
    "#### Layer Normalization\n",
    "\n",
    "## After the first GAT layer, the output is passed through nn.LayerNorm(hidden_channels * num_heads). This helps stabilize training by normalizing feature distributions across different nodes.\n",
    "\n",
    "#### Forward Pass\n",
    "\n",
    "## First apply dropout to the input features (x) to reduce overfitting.\n",
    "## Pass the data through the first GAT layer (gat1).\n",
    "## Apply an ELU activation, then layer normalization, and another dropout.\n",
    "## Finally, pass the features through the second GAT layer (gat2) to get the predictions (the paper category with the highest score).\n",
    "\n",
    "class GAT(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_heads=8, dropout=0.6,add_self_loops=True):\n",
    "        super(GAT, self).__init__()\n",
    "        self.dropout = dropout\n",
    "        # The first layer: multi-head attention, each head outputs hidden_channels, and the total output dimension = hidden_channels * num_heads (i.e. the input dimension of the second layer), you don't need to care the concatenation in this layer\n",
    "        # The usage of GATConv is: GATConv(in_channels, out_channels, heads=XXX, concat=XXX, dropout=dropout, add_self_loops=XXX)\n",
    "        ## TODO 1: Define the first GAT layer (2 points)\n",
    "        \n",
    "        ## TODO 1: Define the first GAT layer\n",
    "        # The second layer: single-head output, not concatenated, directly output the dimension of the node categories\n",
    "        ## TODO 2: Define the second GAT layer (2 points)\n",
    "        \n",
    "        ## TODO 2: Define the second GAT layer\n",
    "        # Layer normalization for stable training, use nn.LayerNorm, the input is the hidden_channels * num_heads\n",
    "        ## TODO 3: Define a LayerNorm layer (2 points)\n",
    "        \n",
    "        ## TODO 3: Define a LayerNorm layer\n",
    "\n",
    "    def forward(self, data):\n",
    "        # data contains x and edge_index\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        ## TODO 4: use a dropout layer for the input features, then apply the first GAT layer (2 points)\n",
    "        # use F.dropout to perform dropout, the input is x, and the dropout rate is self.dropout, set training=self.training\n",
    "\n",
    "\n",
    "        ## TODO 4: use a dropout layer for the input features, then apply the first GAT layer\n",
    "        ## TODO 5: apply ELU activation, layer normalization and dropout (2 points)\n",
    "        # use F.elu for ELU activation, the input is x\n",
    "\n",
    "\n",
    "\n",
    "        ## TODO 5: apply ELU activation, layer normalization and dropout\n",
    "        ## TODO 6: apply the second GAT layer (2 points)\n",
    "\n",
    "        ## TODO 6: apply the second GAT layer\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "# Load the ogbn-arxiv dataset\n",
    "dataset_arxiv = PygNodePropPredDataset(name='ogbn-arxiv')\n",
    "\n",
    "\n",
    "# Let's see some properties of the dataset\n",
    "## TODO 7: print the number of graphs in the dataset (2 points)\n",
    "\n",
    "## TODO 7: print the number of graphs in the dataset\n",
    "\n",
    "# Let's see how many features and classes are in the dataset\n",
    "## TODO 8: print the number of features and classes in the dataset (2 points)\n",
    "\n",
    "## TODO 8: print the number of features and classes in the dataset\n",
    "\n",
    "\n",
    "data_arxiv = dataset_arxiv[0]\n",
    "# Let's see the shape of the node features and the target labels\n",
    "print(data_arxiv.x.shape)\n",
    "\n",
    "# Get the data split index\n",
    "split_idx_arxiv = dataset_arxiv.get_idx_split()\n",
    "train_idx_arxiv = split_idx_arxiv['train']\n",
    "valid_idx_arxiv = split_idx_arxiv['valid']\n",
    "test_idx_arxiv  = split_idx_arxiv['test']\n",
    "\n",
    "# Model, optimizer, scheduler, and evaluator settings\n",
    "model_arxiv = GAT(in_channels=dataset_arxiv.num_features,\n",
    "                         hidden_channels=64,\n",
    "                         out_channels=dataset_arxiv.num_classes,\n",
    "                         num_heads=8,\n",
    "                         dropout=0.6).to(device)\n",
    "data_arxiv = data_arxiv.to(device)\n",
    "optimizer_arxiv = torch.optim.Adam(model_arxiv.parameters(), lr=0.005, weight_decay=5e-4)\n",
    "scheduler_arxiv = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer_arxiv, mode='min', factor=0.7, patience=10, verbose=True)\n",
    "evaluator_arxiv = Evaluator(name='ogbn-arxiv')\n",
    "\n",
    "def train_arxiv():\n",
    "    model_arxiv.train()\n",
    "    optimizer_arxiv.zero_grad()\n",
    "    ## TODO 9: forward propagation, loss calculation and backward propagation (2 points)\n",
    "    # use F.cross_entropy as the loss function, the input is the output of the model and the target labels, and only use the training set for loss calculation, then call loss.backward()\n",
    "\n",
    "\n",
    "\n",
    "    ## TODO 9: forward propagation, loss calculation and backward propagation\n",
    "    optimizer_arxiv.step()\n",
    "    return loss.item()\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_arxiv():\n",
    "    model_arxiv.eval()\n",
    "    ## TODO 10: get the prediction and use argmax to get the final category (2 points)\n",
    "\n",
    "\n",
    "    ## TODO 10: get the prediction and use argmax to get the final category\n",
    "    train_acc = evaluator_arxiv.eval({'y_true': data_arxiv.y[train_idx_arxiv],\n",
    "                                      'y_pred': y_pred[train_idx_arxiv]})['acc']\n",
    "    valid_acc = evaluator_arxiv.eval({'y_true': data_arxiv.y[valid_idx_arxiv],\n",
    "                                      'y_pred': y_pred[valid_idx_arxiv]})['acc']\n",
    "    test_acc  = evaluator_arxiv.eval({'y_true': data_arxiv.y[test_idx_arxiv],\n",
    "                                      'y_pred': y_pred[test_idx_arxiv]})['acc']\n",
    "    return train_acc, valid_acc, test_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a7600734ea1b79",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-10T19:47:13.696601Z",
     "start_time": "2025-03-10T19:47:13.694722Z"
    }
   },
   "outputs": [],
   "source": [
    "## You can change these hyperparameters to see if you can get better results, but the default hyperparameters should work. And also make sure the three sets for hyperparameters are the same.\n",
    "num_epochs = 30\n",
    "best_valid_acc_arxiv = 0\n",
    "patience_arxiv = 30\n",
    "trigger_times_arxiv = 0\n",
    "best_model_state_arxiv = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e896ce1c1aef9bec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-10T19:48:57.548999Z",
     "start_time": "2025-03-10T19:47:13.702337Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"---- ogbn-arxiv training start ----\")\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    loss = train_arxiv()\n",
    "    scheduler_arxiv.step(loss)\n",
    "    train_acc, valid_acc, test_acc = evaluate_arxiv()\n",
    "    ## TODO 11: early stopping, in an if-else block (2 points)\n",
    "    if valid_acc > best_valid_acc_arxiv:\n",
    "\n",
    "\n",
    "\n",
    "    else:\n",
    "       \n",
    "    ## TODO 11: early stopping, in an if-else block\n",
    "\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Train: {train_acc:.4f}, Valid: {valid_acc:.4f}, Test: {test_acc:.4f}')\n",
    "\n",
    "    if trigger_times_arxiv >= patience_arxiv:\n",
    "        print(\"Early stopping triggered!\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c271d5a9c3ee72a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-10T19:48:58.449140Z",
     "start_time": "2025-03-10T19:48:57.577423Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load the best model state\n",
    "model_arxiv.load_state_dict(best_model_state_arxiv)\n",
    "final_train, final_valid, final_test = evaluate_arxiv()\n",
    "print(f\"[ogbn-arxiv] Best validation accuracy: {final_valid:.4f}, corresponding test accuracy: {final_test:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473e14fc034a8236",
   "metadata": {},
   "source": [
    "* What's the best validation accuracy you can get? (2 points) What's the corresponding test accuracy? (2 points) Please report the results in this markdown cell."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861c6925420663ae",
   "metadata": {},
   "source": [
    "* Let's see if we remove the graph structure and only use the node features, how well the model can perform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8510bc73d5baef22",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-10T19:49:34.189297Z",
     "start_time": "2025-03-10T19:48:58.464181Z"
    }
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "# construct a dummy edge_index with self-loops only\n",
    "num_nodes = data_arxiv.num_nodes\n",
    "dummy_edge_index = torch.arange(num_nodes, device=data_arxiv.x.device).unsqueeze(0).repeat(2, 1)\n",
    "\n",
    "# copy the original data and replace the edge_index with the dummy one\n",
    "data_arxiv_no_graph = copy.deepcopy(data_arxiv)\n",
    "data_arxiv_no_graph.edge_index = dummy_edge_index\n",
    "\n",
    "# define a new model for the data without graph structure, with the same hyperparameters\n",
    "model_no_graph = GAT(\n",
    "    in_channels=dataset_arxiv.num_features,\n",
    "    hidden_channels=64,\n",
    "    out_channels=dataset_arxiv.num_classes,\n",
    "    num_heads=8,\n",
    "    dropout=0.6\n",
    ").to(device)\n",
    "\n",
    "\n",
    "optimizer_no_graph = torch.optim.Adam(model_no_graph.parameters(), lr=0.005, weight_decay=5e-4)\n",
    "scheduler_no_graph = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer_no_graph, mode='min', factor=0.7, patience=10, verbose=True\n",
    ")\n",
    "\n",
    "def train_no_graph():\n",
    "    ## TODO 12: training code for the model without graph structure, should be the same as the original training code (2 points)\n",
    "\n",
    "\n",
    "    # use the same model but with data without graph structure\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ## TODO 12: training code for the model without graph structure, should be the same as the original training code\n",
    "    return loss.item()\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_no_graph():\n",
    "    ## TODO 13: evaluation code for the model without graph structure, should be the same as the original evaluation code (2 points)\n",
    "\n",
    "    ## TODO 13: evaluation code for the model without graph structure, should be the same as the original evaluation code\n",
    "    return train_acc, valid_acc, test_acc\n",
    "\n",
    "\n",
    "## You can change these hyperparameters to see if you can get better results, but the default hyperparameters should work. And also make sure the three sets for hyperparameters are the same.\n",
    "num_epochs_no_graph = 30\n",
    "best_valid_acc_arxiv_no_graph = 0\n",
    "patience_arxiv_no_graph = 30\n",
    "trigger_times_arxiv_no_graph = 0\n",
    "best_model_state_arxiv_no_graph = None\n",
    "# start training and evaluation\n",
    "for epoch in range(1, num_epochs_no_graph+1):\n",
    "    loss = train_no_graph()\n",
    "    scheduler_no_graph.step(loss)\n",
    "    train_acc, valid_acc, test_acc = evaluate_no_graph()\n",
    "    ## TODO 14: early stopping, in an if-else block (2 points)\n",
    "    if valid_acc > best_valid_acc_arxiv_no_graph:\n",
    "\n",
    "\n",
    "\n",
    "    else:\n",
    "        \n",
    "    ## TODO 14: early stopping, in an if-else block\n",
    "\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Train: {train_acc:.4f}, Valid: {valid_acc:.4f}, Test: {test_acc:.4f}')\n",
    "\n",
    "    if trigger_times_arxiv_no_graph >= patience_arxiv_no_graph:\n",
    "        print(\"Early stopping triggered!\")\n",
    "        break\n",
    "# Load the best model state\n",
    "model_no_graph.load_state_dict(best_model_state_arxiv_no_graph)\n",
    "final_train, final_valid, final_test = evaluate_no_graph()\n",
    "print(f\"[ogbn-arxiv_no_graph] Best validation accuracy: {final_valid:.4f}, corresponding test accuracy: {final_test:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e2af5f42b0b57d",
   "metadata": {},
   "source": [
    "* What's the best validation accuracy you can get? (2 points) What's the corresponding test accuracy? (2 points) Please report the results in this markdown cell."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574446b1e446f890",
   "metadata": {},
   "source": [
    "* Let's see if we further remove the edge structure and only use the node features, how well the model can perform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99a3de7c982f410",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-10T19:50:08.093156Z",
     "start_time": "2025-03-10T19:49:34.212367Z"
    }
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "# an empty edge_index, even no self-loops\n",
    "dummy_edge_index = torch.empty((2, 0), dtype=torch.long, device=data_arxiv.x.device)\n",
    "\n",
    "# duplicate the original data and replace the edge_index with the dummy one\n",
    "data_arxiv_no_edge = copy.deepcopy(data_arxiv)\n",
    "data_arxiv_no_edge.edge_index = dummy_edge_index\n",
    "\n",
    "# define a new model for the data without graph structure, with the same hyperparameters\n",
    "model_no_edge = GAT(\n",
    "    in_channels=dataset_arxiv.num_features,\n",
    "    hidden_channels=64,\n",
    "    out_channels=dataset_arxiv.num_classes,\n",
    "    num_heads=8,\n",
    "    dropout=0.6\n",
    ").to(device)\n",
    "\n",
    "optimizer_no_edge = torch.optim.Adam(model_no_edge.parameters(), lr=0.005, weight_decay=5e-4)\n",
    "scheduler_no_edge = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer_no_edge, mode='min', factor=0.7, patience=10, verbose=True\n",
    ")\n",
    "\n",
    "def train_no_edge():\n",
    "    ## TODO 15: training code for the model without edge structure, should be the same as the original training code (2 points)\n",
    "\n",
    "\n",
    "    # use the same model but with data without graph structure\n",
    "\n",
    "\n",
    "\n",
    "    ## TODO 15: training code for the model without edge structure, should be the same as the original training code\n",
    "    return loss.item()\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_no_edge():\n",
    "    ## TODO 16: evaluation code for the model without edge structure, should be the same as the original evaluation code (2 points)\n",
    "\n",
    "    ## TODO 16: evaluation code for the model without edge structure, should be the same as the original evaluation code\n",
    "    return train_acc, valid_acc, test_acc\n",
    "\n",
    "\n",
    "## You can change these hyperparameters to see if you can get better results, but the default hyperparameters should work. And also make sure the three sets for hyperparameters are the same.\n",
    "num_epochs_no_edge = 30\n",
    "best_valid_acc_no_edge = 0\n",
    "patience_no_edge = 30\n",
    "trigger_times_no_edge = 0\n",
    "best_model_state_no_edge = None\n",
    "\n",
    "# start training and evaluation\n",
    "for epoch in range(1, num_epochs_no_edge+1):\n",
    "    loss = train_no_edge()\n",
    "    scheduler_no_edge.step(loss)\n",
    "    train_acc, valid_acc, test_acc = evaluate_no_edge()\n",
    "    ## TODO 17: early stopping, in an if-else block (2 points)\n",
    "    if valid_acc > best_valid_acc_no_edge:\n",
    "\n",
    "\n",
    "\n",
    "    else:\n",
    "\n",
    "    ## TODO 17: early stopping, in an if-else block\n",
    "\n",
    "    print(f\"Epoch: {epoch:03d}, Loss: {loss:.4f}, Train: {train_acc:.4f}, Valid: {valid_acc:.4f}, Test: {test_acc:.4f}\")\n",
    "\n",
    "    if trigger_times_no_edge >= patience_no_edge:\n",
    "        print(\"Early stopping triggered!\")\n",
    "        break\n",
    "\n",
    "# load the best model state\n",
    "model_no_edge.load_state_dict(best_model_state_no_edge)\n",
    "final_train, final_valid, final_test = evaluate_no_edge()\n",
    "print(f\"[ogbn-arxiv_no_edge] Best validation accuracy: {final_valid:.4f}, corresponding test accuracy: {final_test:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c182c6357029e6",
   "metadata": {},
   "source": [
    "* What's the best validation accuracy you can get? What's the corresponding test accuracy? Please report the results in this markdown cell (2 points).\n",
    "* What's your observation of the results (all the three situations)? Please write down your observation in this markdown cell (2 points)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df05ed3c",
   "metadata": {},
   "source": [
    "## Graph Convolutional Network (GCN) Implementation for Graph Classification\n",
    "\n",
    "**Objective:** Implement a GCN from scratch to perform graph classification using an OGB dataset. Develop neural components, including the forward pass, as well as training and testing routines. There are 11 `todo`s and 7 questions for GCN.\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "Provide an overview of Graph Neural Networks (GNNs) and the significance of GCNs in processing graph-structured data. Discuss the relevance of graph classification tasks and the role of the OGB datasets in benchmarking.\n",
    "\n",
    "## 2. Dataset Exploration\n",
    "\n",
    "### 2.1. Importing Libraries\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b75bc96aa0e8457",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-10T19:50:08.142940Z",
     "start_time": "2025-03-10T19:50:08.096087Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import DataLoader\n",
    "from ogb.graphproppred import PygGraphPropPredDataset, Evaluator\n",
    "import torch_geometric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1159e827",
   "metadata": {},
   "source": [
    "### 2.2. Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b5c13357b23e0d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-10T19:50:08.199196Z",
     "start_time": "2025-03-10T19:50:08.166681Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "from ogb.graphproppred import PygGraphPropPredDataset  # OGB dataset loader for graph property prediction\n",
    "from torch_geometric.data import DataLoader  # PyG DataLoader for handling batches of graphs\n",
    "\n",
    "# Load the OGB dataset for molecular property prediction.\n",
    "# 'ogbg-molhiv' is a graph-level dataset where each graph represents a molecular structure,\n",
    "# and the task is binary classification to predict whether the molecule inhibits HIV replication.\n",
    "dataset = PygGraphPropPredDataset(name='ogbg-molhiv')\n",
    "\n",
    "# Split the dataset into training, validation, and test sets.\n",
    "# The dataset provides predefined splits to ensure consistency in evaluation.\n",
    "split_idx = dataset.get_idx_split()\n",
    "train_dataset = dataset[split_idx['train']]  # Training set\n",
    "valid_dataset = dataset[split_idx['valid']]  # Validation set\n",
    "test_dataset = dataset[split_idx['test']]    # Test set\n",
    "\n",
    "# Create DataLoaders for batch processing.\n",
    "# The DataLoader enables efficient loading of graphs in batches for training and evaluation.\n",
    "# - batch_size: Number of graphs per batch.\n",
    "# - shuffle: Whether to shuffle the dataset at each epoch (only done for training).\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)   # Shuffle for training\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=128, shuffle=False)  # No shuffle for validation\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)    # No shuffle for testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b9c5767405fc08",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-10T19:50:08.226366Z",
     "start_time": "2025-03-10T19:50:08.223587Z"
    }
   },
   "outputs": [],
   "source": [
    "# Display dataset information\n",
    "print(f'Dataset name: {dataset.name}')\n",
    "print(f'Number of graphs: {len(dataset)}')\n",
    "print(f'Number of classes: {dataset.num_tasks}')\n",
    "print(f'Number of node features: {dataset.num_node_features}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0999f3e",
   "metadata": {},
   "source": [
    "## 3. Model Implementation\n",
    "### 3.1. GCN Convolution Layer Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2590fac4e0f3e0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-10T19:50:08.253831Z",
     "start_time": "2025-03-10T19:50:08.250111Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch_geometric.nn import MessagePassing  # Base class for defining message-passing layers\n",
    "from torch_geometric.utils import add_self_loops, degree  # Utilities for graph processing\n",
    "\n",
    "class GCNConv(MessagePassing):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        \"\"\"\n",
    "        Custom implementation of a Graph Convolutional Network (GCN) layer.\n",
    "\n",
    "        Args:\n",
    "            in_channels (int): Number of input node features.\n",
    "            out_channels (int): Number of output node features.\n",
    "\n",
    "        The layer follows the formulation:\n",
    "            H' = σ(D^(-1/2) A D^(-1/2) H W)\n",
    "        where:\n",
    "            - A is the adjacency matrix with self-loops.\n",
    "            - D is the degree matrix.\n",
    "            - H is the input node feature matrix.\n",
    "            - W is the trainable weight matrix.\n",
    "            - σ is a non-linearity (like ReLU).\n",
    "        \"\"\"\n",
    "        super(GCNConv, self).__init__(aggr='add')  # \"Add\" aggregation means summing messages from neighbors.\n",
    "\n",
    "        # TODO 1: Define a linear transformation layer for node features (2 points)\n",
    "        self.linear = nn.Linear(______, ______)\n",
    "        # TODO 1: Define a linear transformation layer for node features\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        \"\"\"\n",
    "        Forward pass of the GCN layer.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): Node feature matrix of shape [num_nodes, in_channels].\n",
    "            edge_index (Tensor): Graph connectivity in COO format, shape [2, num_edges].\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Updated node features of shape [num_nodes, out_channels].\n",
    "        \"\"\"\n",
    "\n",
    "        # TODO 2: Add self-loops to the adjacency matrix (2 points)\n",
    "        edge_index, _ = add_self_loops(______, num_nodes=______)\n",
    "        # TODO 2: Add self-loops to the adjacency matrix\n",
    "\n",
    "        # TODO 3: Compute node degrees (2 points)\n",
    "        row, col = edge_index  # Extract row (source nodes) and col (destination nodes)\n",
    "        deg = degree(______, x.size(0), dtype=x.dtype)  # Compute degree of each node\n",
    "        deg_inv_sqrt = deg.pow(-0.5)  # Compute D^(-1/2)\n",
    "        # TODO 3: Compute node degrees \n",
    "\n",
    "        # Prevent division by zero for isolated nodes\n",
    "        deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0  \n",
    "\n",
    "        # TODO 4: Compute normalized adjacency matrix (2 points)\n",
    "        norm = ______\n",
    "        # TODO 4: Compute normalized adjacency matrix \n",
    "\n",
    "        # TODO 5: Perform message passing using PyTorch Geometric's propagate function (2 points)\n",
    "        out = self.propagate(______, x=x, norm=norm)\n",
    "        # TODO 5: Perform message passing using PyTorch Geometric's propagate function \n",
    "\n",
    "        # TODO 6: Apply the linear transformation after aggregation (2 points)\n",
    "        out = ______\n",
    "        # TODO 6: Apply the linear transformation after aggregation \n",
    "\n",
    "        return out\n",
    "\n",
    "    def message(self, x_j, norm):\n",
    "        \"\"\"\n",
    "        Message function: Defines how information is aggregated from neighboring nodes.\n",
    "\n",
    "        Args:\n",
    "            x_j (Tensor): Features of neighboring nodes.\n",
    "            norm (Tensor): Normalization coefficients computed from node degrees.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Normalized node features.\n",
    "        \"\"\"\n",
    "        # TODO 7: Apply normalization to the node features (2 points)\n",
    "        return ______\n",
    "        # TODO 7: Apply normalization to the node features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb87f36",
   "metadata": {},
   "source": [
    "## TODO:\n",
    "### Why is it important to add self-loops in the graph convolution process, and how does the normalization strategy here help stabilize training in graph neural networks? (2 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd50c82",
   "metadata": {},
   "source": [
    "### 3.2. Implement GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f2f3f44c367f1a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-10T19:50:08.280885Z",
     "start_time": "2025-03-10T19:50:08.278130Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, num_node_features, hidden_channels, num_classes, dropout_rate):\n",
    "        \"\"\"\n",
    "        Graph Convolutional Network (GCN) for graph classification.\n",
    "\n",
    "        Args:\n",
    "            num_node_features (int): Number of input node features.\n",
    "            hidden_channels (int): Number of hidden units in the GCN layers.\n",
    "            num_classes (int): Number of output classes (for classification tasks).\n",
    "            dropout_rate (float): Dropout probability for regularization.\n",
    "\n",
    "        This model consists of:\n",
    "        - Two GCNConv layers to extract graph structure-aware features.\n",
    "        - ReLU activation and dropout for regularization.\n",
    "        - A global mean pooling layer to aggregate node features into graph-level representations.\n",
    "        - A final linear layer for classification.\n",
    "        \"\"\"\n",
    "        super(GCN, self).__init__()\n",
    "\n",
    "        self.dropout_rate = dropout_rate  # Store dropout rate\n",
    "\n",
    "        # TODO 8: Complete the network layer definitions (2 points)\n",
    "        # First graph convolution layer: transforms node features to hidden dimension\n",
    "        # Second graph convolution layer: refines node embeddings\n",
    "        # Final linear layer to produce class predictions from the pooled graph representation\n",
    "        # TODO 8: Complete the network layer definitions (2 points)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        \"\"\"\n",
    "        Forward pass through the GCN.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): Node feature matrix of shape [num_nodes, num_node_features].\n",
    "            edge_index (Tensor): Graph connectivity in COO format [2, num_edges].\n",
    "            batch (Tensor): Batch index for each node, used for global pooling.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Output class logits of shape [num_graphs, num_classes].\n",
    "        \"\"\"\n",
    "\n",
    "        # TODO 9: Complete the implementation of the forward function (2 points)\n",
    "\n",
    "        # First GCN layer: apply convolution, activation, and dropout\n",
    "        # ReLU activation layer\n",
    "        # Dropout layer for regularization\n",
    "        # Second GCN layer\n",
    "        # Global mean pooling to aggregate node features into a graph \n",
    "        #   representation (Output shape: [num_graphs, hidden_channels])\n",
    "        # Final linear layer for classification (Output shape: [num_graphs, num_classes])\n",
    "        # TODO 9: Complete the implementation of the forward function (2 points)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f28461",
   "metadata": {},
   "source": [
    "## 4. Model Training\n",
    "### 4.1. Implement Training and Evalutation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877b38046824a623",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-10T19:50:08.306599Z",
     "start_time": "2025-03-10T19:50:08.304235Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(model, loader, device):\n",
    "    \"\"\"\n",
    "    Training function for the GCN model.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The GCN model.\n",
    "        loader (DataLoader): DataLoader for the training set.\n",
    "        criterion (torch.nn.Module): Loss function.\n",
    "        optimizer (torch.optim.Optimizer): Optimizer for training.\n",
    "        device (torch.device): Device to run computations on (CPU or GPU).\n",
    "\n",
    "    Returns:\n",
    "        float: Average training loss per graph.\n",
    "    \"\"\"\n",
    "\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    # Iterate over batches in the DataLoader\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "\n",
    "        # TODO 10: Complete the training function (2 points)\n",
    "        #  Zero the gradients of the Adam optimizer from `torch.optim`\n",
    "\n",
    "        # Perform a forward pass through the model\n",
    "\n",
    "        # Compute the loss use `BCEWithLogitsLoss()``\n",
    "\n",
    "        #  Perform backpropagation\n",
    "\n",
    "        # Update model parameters\n",
    "\n",
    "        # TODO 10: Complete the training function\n",
    "\n",
    "        # Accumulate loss\n",
    "        total_loss += loss.item() * data.num_graphs\n",
    "\n",
    "    return total_loss / len(loader.dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e375aab5",
   "metadata": {},
   "source": [
    "## TODO:\n",
    "### How does the choice of the BCEWithLogitsLoss and the Adam optimizer influence the training dynamics in this setting? (2 points)\n",
    "### What are potential alternative choices and their pros/cons for graph-based binary classification tasks? (2 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c670abd70eec8c8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-10T19:50:08.353147Z",
     "start_time": "2025-03-10T19:50:08.350233Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate(model, loader, evaluator, device):\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            data = data.to(device)\n",
    "            out = model(data.x, data.edge_index, data.batch)\n",
    "            y_true.append(data.y.view(-1, 1))\n",
    "            y_pred.append(out)\n",
    "    y_true = torch.cat(y_true, dim=0)\n",
    "    y_pred = torch.cat(y_pred, dim=0)\n",
    "    return evaluator.eval({'y_true': y_true, 'y_pred': y_pred})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57b8539",
   "metadata": {},
   "source": [
    "## TODO:\n",
    "### What potential pitfalls can arise during the evaluation phase of graph neural networks? (2 points)\n",
    "### How might one detect issues like overfitting or underfitting using evaluation metrics such as ROC-AUC? (2 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b53dde",
   "metadata": {},
   "source": [
    "### 4.2. Training Loop (Hyper-parameter Tuning to reach `Validation AUC` $\\ge$ 0.7 ) \n",
    "\n",
    "1 pt: `Validation AUC` $\\ge$ 0.69\n",
    "\n",
    "\n",
    "1.5 pts: `Validation AUC` $\\ge$ 0.71\n",
    "\n",
    "\n",
    "2 pts: `Validation AUC` $\\ge$ 0.73"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a5d354221345e0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-10T19:50:08.360922Z",
     "start_time": "2025-03-10T19:50:08.356358Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO 11: Tune your hyper-parameter to achieve high performance (2 points)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = GCN(num_node_features=dataset.num_node_features, hidden_channels=32, num_classes=dataset.num_tasks, dropout_rate = 0.5).to(device)\n",
    "evaluator = Evaluator(name='ogbg-molhiv')\n",
    "# TODO 11: Tune your hyper-parameter to achieve high performance\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4ade1304643e31",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-03-10T19:50:08.383926Z"
    },
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "num_epochs = 5\n",
    "best_valid_auc = 0\n",
    "\n",
    "for epoch in tqdm(range(1, num_epochs + 1)):\n",
    "    # Training\n",
    "    train_loss = train(model, train_loader, device)\n",
    "\n",
    "    # Evaluation\n",
    "    train_result = evaluate(model, train_loader, evaluator, device)\n",
    "    valid_result = evaluate(model, valid_loader, evaluator, device)\n",
    "\n",
    "    train_auc = train_result['rocauc']\n",
    "    valid_auc = valid_result['rocauc']\n",
    "\n",
    "    # Print metrics\n",
    "    print(f'Epoch: {epoch:03d}, '\n",
    "          f'Train Loss: {train_loss:.4f}, '\n",
    "          f'Train AUC: {train_auc:.4f}, '\n",
    "          f'Validation AUC: {valid_auc:.4f}')\n",
    "\n",
    "    # Save the best model\n",
    "    if valid_auc > best_valid_auc:\n",
    "        best_valid_auc = valid_auc\n",
    "        print(f'Best model with Validation AUC: {best_valid_auc:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f28247b",
   "metadata": {},
   "source": [
    "### OPEN ENDED QUESTION:\n",
    "#### Reflect on the entire implementation of the GCN model for molecular property prediction.\n",
    "#### - What potential improvements or modifications could you suggest to enhance the model's performance? Consider aspects such as network architecture, hyperparameter tuning, data preprocessing, and evaluation strategies. (2 points)\n",
    "#### - How might you leverage additional graph-specific techniques or modern architectures to push the boundaries of performance on this task? (2 points)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
