{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment 2: Implementing and Comparing RNN & LSTM on MNIST\n",
        "\n",
        "In this assignment, you will:\n",
        "\n",
        "1. **Review** how a simple Recurrent Neural Network (RNN) processes sequential data.\n",
        "2. **Implement** both an RNN and an LSTM from scratch (using NumPy), and train each model on the MNIST dataset.\n",
        "3. **Compare** their training performance and final accuracy using plots of losses, accuracies, and confusion matrices.\n",
        "\n",
        "**Key Skills:**\n",
        "- Defining a basic RNN and LSTM architecture in Python\n",
        "- Backpropagation through time for parameter updates\n",
        "- Using the MNIST dataset for image based classification in a sequential setting\n",
        "- Visualizing and analyzing results using performance curves and confusion matrices\n",
        "\n",
        "Feel free to experiment with:\n",
        "- Changing hyperparameters (learning rate, hidden dimension, etc.)\n",
        "- Different subsets of the training data\n",
        "- Additional analysis of misclassifications\n",
        "\n",
        "Your goal is to observe how LSTMs address the limitations of RNNs (particularly vanishing gradients) and potentially achieve better performance on longer sequences.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "hvOZPVyfK5bn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================================\n",
        "# Imports and Global Setup\n",
        "# ========================================\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import sklearn.metrics\n",
        "import seaborn as sns\n",
        "import random\n",
        "\n",
        "def set_seed(seed=27):\n",
        "    '''\n",
        "    Sets the seed for this assignment so results\n",
        "    are the same every time we run.\n",
        "    '''\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "\n",
        "    # When running on the CuDNN backend, two further options must be set\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    # Set a fixed value for the hash seed\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "\n",
        "set_seed()\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Device available now:', device)\n"
      ],
      "metadata": {
        "id": "rEGQYPMdX9_c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================================\n",
        "# helper functions\n",
        "# ========================================\n",
        "# Contains utility functions for analyzing and plotting results.\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def get_confusion_matrix(model, x_test, y_test):\n",
        "    \"\"\"\n",
        "    Generate a confusion matrix for the given model and test data.\n",
        "\n",
        "    Args:\n",
        "        model: The trained RNN or LSTM model\n",
        "        x_test: Test images\n",
        "        y_test: Test labels\n",
        "\n",
        "    Returns:\n",
        "        confusion_matrix: A 10x10 confusion matrix\n",
        "    \"\"\"\n",
        "    predictions = []\n",
        "    actual_labels = []\n",
        "\n",
        "    # Get predictions for all test samples\n",
        "    for i in range(len(x_test)):\n",
        "        # Forward pass\n",
        "        outputs, _ = model.forward(x_test[i])\n",
        "\n",
        "        # Get predicted class\n",
        "        pred_class = np.argmax(outputs)\n",
        "\n",
        "        # Store prediction and actual label\n",
        "        predictions.append(pred_class)\n",
        "        actual_labels.append(y_test[i])\n",
        "\n",
        "    # Generate confusion matrix\n",
        "    conf_matrix = confusion_matrix(actual_labels, predictions, labels=range(10))\n",
        "\n",
        "    return conf_matrix\n",
        "\n",
        "def analyze_confusion_matrix(conf_matrix, model_name):\n",
        "    \"\"\"\n",
        "    Print analysis of the confusion matrix\n",
        "\n",
        "    Args:\n",
        "        conf_matrix: The confusion matrix to analyze\n",
        "        model_name: Name of the model (for reporting)\n",
        "    \"\"\"\n",
        "    # Calculate per-class accuracy\n",
        "    class_accuracy = np.zeros(10)\n",
        "    for i in range(10):\n",
        "        class_accuracy[i] = conf_matrix[i, i] / np.sum(conf_matrix[i, :]) * 100\n",
        "\n",
        "    # Find the classes with highest and lowest accuracy\n",
        "    best_class = np.argmax(class_accuracy)\n",
        "    worst_class = np.argmin(class_accuracy)\n",
        "\n",
        "    print(f\"\\n{model_name} Confusion Matrix Analysis:\")\n",
        "    print(f\"Best recognized digit: {best_class} with {class_accuracy[best_class]:.2f}% accuracy\")\n",
        "    print(f\"Worst recognized digit: {worst_class} with {class_accuracy[worst_class]:.2f}% accuracy\")\n",
        "\n",
        "    # Find common misclassifications\n",
        "    misclassifications = []\n",
        "    for i in range(10):\n",
        "        for j in range(10):\n",
        "            if i != j and conf_matrix[i, j] > 0:\n",
        "                misclassifications.append((i, j, conf_matrix[i, j]))\n",
        "\n",
        "    # Sort by number of occurrences\n",
        "    misclassifications.sort(key=lambda x: x[2], reverse=True)\n",
        "\n",
        "    print(\"\\nTop 3 misclassifications:\")\n",
        "    for i in range(min(3, len(misclassifications))):\n",
        "        print(f\"  {misclassifications[i][0]} confused as {misclassifications[i][1]}: {misclassifications[i][2]} times\")\n",
        "\n",
        "def plot_confusion_matrices(rnn_model, lstm_model, x_test, y_test):\n",
        "    \"\"\"\n",
        "    Create and save confusion matrix plots for both RNN and LSTM models\n",
        "\n",
        "    Args:\n",
        "        rnn_model: Trained RNN model\n",
        "        lstm_model: Trained LSTM model\n",
        "        x_test: Test data features\n",
        "        y_test: Test data labels\n",
        "    \"\"\"\n",
        "    # Create a figure with two subplots side by side\n",
        "    plt.figure(figsize=(20, 8))\n",
        "\n",
        "    # RNN confusion matrix\n",
        "    plt.subplot(1, 2, 1)\n",
        "    rnn_conf_matrix = get_confusion_matrix(rnn_model, x_test, y_test)\n",
        "    sns.heatmap(rnn_conf_matrix, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=range(10), yticklabels=range(10))\n",
        "    plt.title('Confusion Matrix: RNN', fontsize=15)\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.ylabel('True Label')\n",
        "\n",
        "    # LSTM confusion matrix\n",
        "    plt.subplot(1, 2, 2)\n",
        "    lstm_conf_matrix = get_confusion_matrix(lstm_model, x_test, y_test)\n",
        "    sns.heatmap(lstm_conf_matrix, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=range(10), yticklabels=range(10))\n",
        "    plt.title('Confusion Matrix: LSTM', fontsize=15)\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.ylabel('True Label')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('confusion_matrices.png')\n",
        "    print(\"Confusion matrices saved to 'confusion_matrices.png'\")\n",
        "\n",
        "    # Print analysis\n",
        "    analyze_confusion_matrix(rnn_conf_matrix, \"RNN\")\n",
        "    analyze_confusion_matrix(lstm_conf_matrix, \"LSTM\")\n",
        "\n",
        "    return rnn_conf_matrix, lstm_conf_matrix\n",
        "\n",
        "def plot_performance_curves(rnn_results, lstm_results):\n",
        "    \"\"\"\n",
        "    Create and save plots of training and testing metrics\n",
        "\n",
        "    Args:\n",
        "        rnn_results: Dictionary containing RNN training results\n",
        "        lstm_results: Dictionary containing LSTM training results\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(12, 10))\n",
        "\n",
        "    plt.subplot(2, 2, 1)\n",
        "    plt.plot(rnn_results['train_losses'], label='RNN Training Loss')\n",
        "    plt.plot(lstm_results['train_losses'], label='LSTM Training Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Training Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(2, 2, 2)\n",
        "    plt.plot(rnn_results['test_losses'], label='RNN Test Loss')\n",
        "    plt.plot(lstm_results['test_losses'], label='LSTM Test Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Test Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(2, 2, 3)\n",
        "    plt.plot(rnn_results['train_accuracies'], label='RNN Training Accuracy')\n",
        "    plt.plot(lstm_results['train_accuracies'], label='LSTM Training Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy (%)')\n",
        "    plt.title('Training Accuracy')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(2, 2, 4)\n",
        "    plt.plot(rnn_results['test_accuracies'], label='RNN Test Accuracy')\n",
        "    plt.plot(lstm_results['test_accuracies'], label='LSTM Test Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy (%)')\n",
        "    plt.title('Test Accuracy')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('model_comparison_plots.png')\n",
        "    print(\"Performance plots saved to 'model_comparison_plots.png'\")\n",
        "\n",
        "\n",
        "def rel_error(x, y, eps=1e-10):\n",
        "    \"\"\"Return maximum relative error between x and y.\"\"\"\n",
        "    return np.max(np.abs(x - y) / np.maximum(eps, np.abs(x) + np.abs(y)))\n",
        "\n",
        "def eval_numerical_gradient_array(f, x, df=1.0, h=1e-5):\n",
        "    \"\"\"\n",
        "    Numerical gradient of f w.r.t x.\n",
        "    Assumes x is a numpy array and f(x) returns a scalar.\n",
        "    \"\"\"\n",
        "    grad = np.zeros_like(x)\n",
        "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
        "    while not it.finished:\n",
        "        idx = it.multi_index\n",
        "        old_val = x[idx]\n",
        "\n",
        "        # f(x + h)\n",
        "        x[idx] = old_val + h\n",
        "        fxph = f(x)\n",
        "        # f(x - h)\n",
        "        x[idx] = old_val - h\n",
        "        fxmh = f(x)\n",
        "\n",
        "        grad[idx] = (fxph - fxmh) / (2 * h)\n",
        "        x[idx] = old_val  # restore\n",
        "        it.iternext()\n",
        "    return grad"
      ],
      "metadata": {
        "id": "UtMYtPn9cZNv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "From Now, we will walk through:\n",
        "\n",
        "1. Basic RNN concepts  \n",
        "2. Implementing a simple RNN class  \n",
        "3. Applying an RNN to the MNIST dataset  \n",
        "4. Comparing RNN vs. LSTM for performance  \n",
        "\n",
        "**Your task** is to fill in any `# TODO` parts and experiment with different hyperparameters, data subsets, and analysis. ***Two points per `# TODO`***\n"
      ],
      "metadata": {
        "id": "LdMHnl4acLQJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Simple RNN (Single Layer)\n",
        "**Recurrent Neural Networks** (RNNs) differ from classic **feed forward networks** (FNNs) or **convolutional networks** (CNNs) in how they handle sequence data. Instead of processing each input independently, they maintain a hidden state that carries information from previous time steps.\n",
        "\n",
        "Key points about a 1-layer RNN:\n",
        "\n",
        "1. The network can use previous information to affect the current output.\n",
        "2. It has three conceptual parts: an Input layer, a Hidden state, and an Output layer.\n",
        "3. It processes each element (time step) in a sequence, carrying forward the hidden state.\n",
        "4. The hidden state is updated at each time step via learnable matrices – typically noted (in some references) as U for input, W for hidden recurrent connections, and V for output. In our code, they appear slightly differently.\n",
        "5. Because these weight matrices are reused for all time steps, an RNN can (in theory) handle sequences of arbitrary length.\n",
        "\n",
        "\n",
        "Here’s a common diagram of an RNN unrolled over time:\n",
        "\n",
        "<img src=\"https://i.imgur.com/RW41Wqj.png\" width=\"600\">\n",
        "\n",
        "obtained from https://www.kaggle.com/code/andradaolteanu/pytorch-rnns-and-lstms-explained-acc-0-99"
      ],
      "metadata": {
        "id": "8dIaZ-KPcTKx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================\n",
        "# Single-Layer RNN\n",
        "# ===========================\n",
        "import numpy as np\n",
        "\n",
        "class RNN:\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        \"\"\"\n",
        "        TODO 1:\n",
        "          - Initialize RNN parameters with the correct shapes.\n",
        "          - self.input_dim, self.hidden_dim, self.output_dim are given.\n",
        "          - Hint: The scale factor is 0.1, for example.\n",
        "        \"\"\"\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "        # TODO 1.1: Weight initialization\n",
        "        pass # Recurrent weights\n",
        "        pass # Input weights\n",
        "        pass # Output weights\n",
        "\n",
        "        # TODO 1.2: Bias initialization (zeros)\n",
        "        pass  # Bias for hidden\n",
        "        pass  # Bias for output\n",
        "\n",
        "    def tanh(self, x):\n",
        "        \"\"\"\n",
        "        TODO 2: Implement tanh for the RNN\n",
        "        \"\"\"\n",
        "        # TODO 2.1 Return Tanh function\n",
        "        pass\n",
        "\n",
        "    def forward(self, X):\n",
        "        \"\"\"\n",
        "        TODO 3: Forward pass for the RNN.\n",
        "\n",
        "        Args:\n",
        "          X shape: (sequence_length, input_dim)\n",
        "          e.g. For a flattened row of 28 pixels, sequence_length=28, input_dim=28\n",
        "        Returns:\n",
        "          y: final output\n",
        "          h_t: final hidden state\n",
        "        \"\"\"\n",
        "        # TODO 3.1 Initialize hidden state\n",
        "        pass\n",
        "\n",
        "        # 3.2 Cache\n",
        "        self.cache = []\n",
        "\n",
        "        # TODO 3.3 Process each time step\n",
        "        for t in range(X.shape[0]):\n",
        "            # Reshape the input row into a column\n",
        "            pass\n",
        "\n",
        "            # Compute new hidden state\n",
        "            pass\n",
        "\n",
        "            # Append to cache for backprop\n",
        "            pass\n",
        "\n",
        "            # Move forward in time\n",
        "            pass\n",
        "\n",
        "        # TODO 3.4 Final output after last time step\n",
        "        pass\n",
        "\n",
        "        return y, h_t\n",
        "\n",
        "    def backward(self, dLdy, learning_rate=0.001, clip_value=5.0):\n",
        "        \"\"\"\n",
        "        TODO 4: Backpropagation through time (BPTT).\n",
        "\n",
        "        dLdy: gradient of the loss w.r.t. final output\n",
        "        learning_rate: step size for updates\n",
        "        clip_value: gradient clipping threshold\n",
        "        \"\"\"\n",
        "        # 4.1 Initialize partial derivatives\n",
        "        dWh = np.zeros_like(self.Wh)\n",
        "        dWx = np.zeros_like(self.Wx)\n",
        "        dWy = np.zeros_like(self.Wy)\n",
        "        dbh = np.zeros_like(self.bh)\n",
        "        dby = np.zeros_like(self.by)\n",
        "\n",
        "        # 4.2 Make sure dLdy has correct shape\n",
        "        dLdy = dLdy.reshape(self.by.shape)\n",
        "\n",
        "        # TODO 4.3 Gradient of final output\n",
        "        pass\n",
        "        # The last hidden state is stored in self.cache[-1][2]\n",
        "        dWy = np.dot(dLdy, self.cache[-1][2].T)\n",
        "\n",
        "        # TODO 4.4 Flow back to hidden\n",
        "        pass\n",
        "\n",
        "        # 4.5 Loop backward over time\n",
        "        for t in reversed(range(len(self.cache))):\n",
        "            h_prev, x_t, h_t = self.cache[t]\n",
        "\n",
        "            # TODO\n",
        "            # gradient of tanh\n",
        "            pass\n",
        "            # update weights and bias\n",
        "            pass\n",
        "\n",
        "            if t > 0:\n",
        "                # hidden state from previous time step\n",
        "                # TODO\n",
        "                pass\n",
        "            else:\n",
        "                pass\n",
        "\n",
        "        # 4.6 Clip gradients to avoid exploding\n",
        "        for grad in [dWh, dWx, dWy, dbh, dby]:\n",
        "            np.clip(grad, -clip_value, clip_value, out=grad)\n",
        "\n",
        "        # 4.7 Update parameters\n",
        "        self.Wh -= learning_rate * dWh\n",
        "        self.Wx -= learning_rate * dWx\n",
        "        self.Wy -= learning_rate * dWy\n",
        "        self.bh -= learning_rate * dbh\n",
        "        self.by -= learning_rate * dby\n"
      ],
      "metadata": {
        "id": "GwjACX24cb5-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_rnn_shapes_and_grad(rnn_class):\n",
        "    \"\"\"\n",
        "    Tests:\n",
        "      (A) shape checks for forward pass\n",
        "      (B) numerical gradient check w.r.t. RNN parameters\n",
        "          OR w.r.t input, if your RNN code supports that\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"========== Test RNN layer ==========\")\n",
        "\n",
        "    # (1) Create a small random input X with shape (time_steps, input_dim)\n",
        "    time_steps = 3\n",
        "    input_dim = 4\n",
        "    hidden_dim = 5\n",
        "    output_dim = 2\n",
        "\n",
        "    # For reproducibility\n",
        "    np.random.seed(27)\n",
        "\n",
        "    # Instantiate the RNN\n",
        "    model = rnn_class(input_dim, hidden_dim, output_dim)\n",
        "\n",
        "    # Create random input\n",
        "    X = np.random.randn(time_steps, input_dim)\n",
        "    # Forward pass\n",
        "    y, h_final = model.forward(X)\n",
        "\n",
        "    # =============================\n",
        "    # (A) SHAPE CHECK\n",
        "    # =============================\n",
        "    # The final output y should have shape (output_dim, 1)\n",
        "    # The final hidden state h_final should have shape (hidden_dim, 1)\n",
        "    print(\"Forward output y shape:\", y.shape, f\"(expected {(output_dim, 1)})\")\n",
        "    print(\"Final hidden shape:\", h_final.shape, f\"(expected {(hidden_dim, 1)})\")\n",
        "\n",
        "test_rnn_shapes_and_grad(RNN)"
      ],
      "metadata": {
        "id": "Z-NovnWsQ8wf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## From Basic RNN to LSTM\n",
        "\n",
        "### 1. Why RNNs Struggle on Long Sequences\n",
        "\n",
        "A **Recurrent Neural Network (RNN)** maintains a hidden state \\\\(\\\\bf{h}\\\\) that evolves over time, combining the new input \\\\(\\\\bf{x}_t\\\\) with the previous state \\\\(\\\\bf{h}_{t-1}\\\\). This approach helps RNNs handle sequential data, but they often suffer from **vanishing (or exploding) gradients** when sequences become long.\n",
        "\n",
        "**Vanishing gradients** happen because, during **backpropagation through time (BPTT)**, gradients get multiplied repeatedly by the recurrent weight matrix at each time step. If those weights are small, the gradients can shrink exponentially as you go backward through many timesteps. This often prevents the network from “remembering” early inputs in a long sequence.\n",
        "\n",
        "\n",
        "### 2. LSTM’s Key Idea\n",
        "\n",
        "A **Long Short-Term Memory (LSTM)** network addresses the vanishing gradient problem by introducing a **cell state** \\(\\mathbf{c}_t\\) and a set of **gates** that carefully control the flow of information:\n",
        "\n",
        "1. **Forget Gate** \\\\(\\\\b{f}_t\\\\): Decides what information to **discard** from the cell state.\n",
        "2. **Input Gate** \\\\(\\\\b{i}_t\\\\): Decides what new information to **add** to the cell state via a candidate \\\\(\\\\bf{c}_t^\\sim\\\\).\n",
        "3. **Cell State** \\\\(\\\\b{c}_t\\\\): Combines the previous state with the newly scaled candidate.\n",
        "4. **Output Gate** \\\\(\\\\b{o}_t\\\\): Decides what the **hidden state** \\\\(\\\\b{h}_t\\\\) should be, based on the updated cell state.\n",
        "\n",
        "By selectively forgetting or adding information, LSTM units can preserve important signals for longer, thus **significantly reducing** the problem of vanishing gradients.\n",
        "\n",
        "### 3. Comparing RNN vs. LSTM\n",
        "\n",
        "| Aspect                | Basic RNN                                          | LSTM                                                                  |\n",
        "|-----------------------|----------------------------------------------------|-----------------------------------------------------------------------|\n",
        "| **Hidden State**      | Single hidden state \\\\(\\\\b{h}_t\\\\) only          | Hidden state \\\\(\\\\b{h}_t\\\\) **plus** cell state \\\\(\\\\b{c}_t\\\\)     |\n",
        "| **Memory Mechanism**  | Relies on repeated matrix multiplication           | Uses gating (input, forget, output) to control memory more precisely  |\n",
        "| **Vanishing Gradient**| Likely for long sequences                          | Much less likely, due to explicit cell state and gating               |\n",
        "| **Performance**       | Often good for short or modest sequence lengths    | Typically superior for longer sequences or tasks needing longer memory |\n",
        "\n",
        "### 4. Inside an LSTM Step\n",
        "\n",
        "In code, each time step uses:\n",
        "\n",
        "\\[\n",
        "\\begin{aligned}\n",
        "\\mathbf{i}_t &= \\sigma(W_i \\mathbf{x}_t + U_i \\mathbf{h}_{t-1} + \\mathbf{b}_i), \\\\\n",
        "\\mathbf{f}_t &= \\sigma(W_f \\mathbf{x}_t + U_f \\mathbf{h}_{t-1} + \\mathbf{b}_f), \\\\\n",
        "\\mathbf{o}_t &= \\sigma(W_o \\mathbf{x}_t + U_o \\mathbf{h}_{t-1} + \\mathbf{b}_o), \\\\\n",
        "\\tilde{\\mathbf{c}}_t &= \\tanh(W_c \\mathbf{x}_t + U_c \\mathbf{h}_{t-1} + \\mathbf{b}_c), \\\\\n",
        "\\mathbf{c}_t &= \\mathbf{f}_t \\odot \\mathbf{c}_{t-1} \\;+\\; \\mathbf{i}_t \\odot \\tilde{\\mathbf{c}}_t, \\\\\n",
        "\\mathbf{h}_t &= \\mathbf{o}_t \\odot \\tanh(\\mathbf{c}_t).\n",
        "\\end{aligned}\n",
        "\\]\n",
        "\n",
        "- **\\\\(\\\\b{f}_t\\\\)** (forget gate) decides what fraction of \\\\(\\\\b{c}_{t-1}\\\\) to keep.  \n",
        "- **\\\\(\\\\b{i}_t\\\\)** (input gate) decides how much of the new candidate \\\\(\\\\tilde{\\mathbf{c}}_t\\\\) to add in.  \n",
        "- **\\\\(\\\\b{c}_t\\\\)** (cell state) becomes the new “long-term memory.”  \n",
        "- **\\\\(\\\\b{o}_t\\\\)** (output gate) scales the final hidden state.  \n",
        "\n",
        "### 5. Why LSTM Helps\n",
        "\n",
        "Because the LSTM cell state \\(\\mathbf{c}_t\\) can carry gradients more directly back through time, the model can retain important signals from earlier timesteps—rather than exponentially shrinking them. This gating mechanism makes LSTMs especially powerful for tasks such as language modeling, speech recognition, or any problem where a network must “remember” relevant context over many steps.\n",
        "\n",
        "### 6. Code Overview (Student TODO)\n",
        "\n",
        "In the `LSTM.py - Student TODO Version`, we see:\n",
        "\n",
        "1. **Initialization**: We create separate weight matrices (`Wi`, `Ui`, `Wf`, etc.) for each gate, plus a final `Wy` for outputs.\n",
        "2. **Forward**: We loop over timesteps, computing each gate, updating \\\\(\\\\b{c}_t\\\\), then deriving \\\\(\\\\b{h}_t\\\\). We store these values in `self.cache` for backpropagation.\n",
        "3. **Backward**: We perform BPTT in reverse, using the chain rule to update each gate’s parameters. We also handle **gradient clipping** to avoid exploding gradients.\n",
        "4. **Comparison to RNN**: While an RNN also keeps a hidden state \\\\(\\\\b{h}_t\\\\), it lacks the cell state \\\\(\\\\b{c}_t\\\\) or gating. This means an RNN might quickly forget early inputs in a long sequence. The LSTM, through gating, selectively maintains or discards information, typically leading to better performance on longer tasks.\n",
        "\n",
        "**TL;DR**: LSTM is effectively a more advanced RNN that uses an internal gating mechanism to address vanishing gradients and remember information for longer sequences.\n"
      ],
      "metadata": {
        "id": "1yhNwM_Ec2WT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================================\n",
        "# LSTM\n",
        "# ========================================\n",
        "import numpy as np\n",
        "\n",
        "class LSTM:\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        \"\"\"\n",
        "        TODO 1:\n",
        "            - Initialize LSTM parameters with correct shapes.\n",
        "            - Use a scale factor for random initialization (e.g., scale=0.1).\n",
        "            - The parameters you need are:\n",
        "                Wi, Ui, bi  -> for the input gate\n",
        "                Wf, Uf, bf  -> for the forget gate\n",
        "                Wo, Uo, bo  -> for the output gate\n",
        "                Wc, Uc, bc  -> for the cell state (candidate)\n",
        "                Wy, by      -> for the final output layer\n",
        "        \"\"\"\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "        # 1.1: Scale\n",
        "        scale = 0.1\n",
        "\n",
        "        # TODO 1.2 Initialize Input gate\n",
        "        pass\n",
        "\n",
        "        # TODO 1.3 Initialize Forget gate\n",
        "        pass\n",
        "\n",
        "        # TODO 1.4 Initialize Output gate\n",
        "        pass\n",
        "\n",
        "        # TODO 1.5 Initialize Cell\n",
        "        pass\n",
        "\n",
        "        # TODO 1.6 Initialize Final output layer\n",
        "        pass\n",
        "\n",
        "    def sigmoid(self, x):\n",
        "        \"\"\"\n",
        "        TODO 2: Implement sigmoid in a safe manner, e.g. clipping x\n",
        "                (already done partially with np.clip).\n",
        "        \"\"\"\n",
        "        # TODO 2.1 Return Sigmoid Function\n",
        "        pass\n",
        "\n",
        "    def tanh(self, x):\n",
        "        \"\"\"\n",
        "        TODO 3: Implement or use np.tanh for the LSTM cell candidate & output gating.\n",
        "        \"\"\"\n",
        "        # TODO 3.1 Return Tanh Function\n",
        "        pass\n",
        "\n",
        "    def forward(self, X):\n",
        "        \"\"\"\n",
        "        TODO 4: Forward pass through the LSTM.\n",
        "\n",
        "        X shape: (sequence_length, input_dim)\n",
        "        We'll maintain hidden state (h_prev) and cell state (c_prev),\n",
        "        then compute i_t, f_t, o_t, c_tilde, c_t, h_t for each time step.\n",
        "\n",
        "        Save necessary values in self.cache for BPTT.\n",
        "        Return:\n",
        "          y (final output), shape = (output_dim, 1)\n",
        "          h_t (final hidden state), shape = (hidden_dim, 1)\n",
        "        \"\"\"\n",
        "        # TODO 4.1 : Initialize previous hidden state and previous cell state\n",
        "        pass\n",
        "\n",
        "        # 4.2 Cache\n",
        "        self.cache = []\n",
        "\n",
        "        # TODO 3.3 Process each time step\n",
        "        for t in range(X.shape[0]):\n",
        "            # Reshape the input row into a column\n",
        "            pass\n",
        "\n",
        "            # TODO 3.4 Implement Input gate\n",
        "            pass\n",
        "            # TODO 3.5 Implement Forget gate\n",
        "            pass\n",
        "            # TODO 3.6 Implement Output gate\n",
        "            pass\n",
        "            # TODO 3.7 Implement Candidate\n",
        "            pass\n",
        "            # TODO 3.8 Compute C and H\n",
        "            pass\n",
        "\n",
        "            # Append to cache for backprop\n",
        "            self.cache.append((x_t, h_prev, c_prev, i_t, f_t, o_t, c_tilde, c_t, h_t))\n",
        "            # Move forward in time\n",
        "            h_prev = h_t\n",
        "            c_prev = c_t\n",
        "\n",
        "        # TODO 3.9 Final output after last time step\n",
        "        pass\n",
        "        return y, h_t\n",
        "\n",
        "    def backward(self, dLdy, learning_rate=0.001, clip_value=5.0):\n",
        "        \"\"\"\n",
        "        TODO 5: Backpropagation for LSTM\n",
        "\n",
        "        dLdy: gradient of loss w.r.t. final output\n",
        "        Steps:\n",
        "          1) compute gradient for final output layer (Wy, by)\n",
        "          2) iterate backward over each time step to get i_t, f_t, o_t, c_tilde, etc.\n",
        "          3) clip the gradients\n",
        "          4) update parameters\n",
        "        \"\"\"\n",
        "        # 5.1 Initialize param grads\n",
        "        dWi, dUi, dbi = np.zeros_like(self.Wi), np.zeros_like(self.Ui), np.zeros_like(self.bi)\n",
        "        dWf, dUf, dbf = np.zeros_like(self.Wf), np.zeros_like(self.Uf), np.zeros_like(self.bf)\n",
        "        dWo, dUo, dbo = np.zeros_like(self.Wo), np.zeros_like(self.Uo), np.zeros_like(self.bo)\n",
        "        dWc, dUc, dbc = np.zeros_like(self.Wc), np.zeros_like(self.Uc), np.zeros_like(self.bc)\n",
        "        dWy, dby = np.zeros_like(self.Wy), np.zeros_like(self.by)\n",
        "\n",
        "        # 5.2 Reshape dLdy\n",
        "        dLdy = dLdy.reshape(self.by.shape)\n",
        "\n",
        "        # TODO 5.3: Compute gradient of final output\n",
        "        pass\n",
        "        pass  # last hidden is self.cache[-1][8]\n",
        "\n",
        "        # TODO 5.4 Flow back to Next hidden & cell gradients\n",
        "        pass\n",
        "        pass  # shape: (hidden_dim, 1)\n",
        "\n",
        "        # 5.5 Loop backward over time\n",
        "        for t in reversed(range(len(self.cache))):\n",
        "            (x_t, h_prev, c_prev, i_t, f_t, o_t, c_tilde, c_t, h_t) = self.cache[t]\n",
        "            # TODO 5.6\n",
        "            # Compute partial derivatives\n",
        "            # do, dc, etc.\n",
        "            pass\n",
        "\n",
        "            # TODO 5.7 Update the weights and bias\n",
        "            pass\n",
        "\n",
        "            # TODO 5.8 Compute dx and dh_prev and dc_prev\n",
        "            pass\n",
        "\n",
        "            # TODO 5.9 Move forward in time\n",
        "            pass\n",
        "\n",
        "        # Clip grads\n",
        "        for grad in [dWi, dUi, dbi, dWf, dUf, dbf, dWo, dUo, dbo, dWc, dUc, dbc, dWy, dby]:\n",
        "            np.clip(grad, -clip_value, clip_value, out=grad)\n",
        "\n",
        "        # 5.10: Update parameters\n",
        "        self.Wi -= learning_rate * dWi\n",
        "        self.Ui -= learning_rate * dUi\n",
        "        self.bi -= learning_rate * dbi\n",
        "\n",
        "        self.Wf -= learning_rate * dWf\n",
        "        self.Uf -= learning_rate * dUf\n",
        "        self.bf -= learning_rate * dbf\n",
        "\n",
        "        self.Wo -= learning_rate * dWo\n",
        "        self.Uo -= learning_rate * dUo\n",
        "        self.bo -= learning_rate * dbo\n",
        "\n",
        "        self.Wc -= learning_rate * dWc\n",
        "        self.Uc -= learning_rate * dUc\n",
        "        self.bc -= learning_rate * dbc\n",
        "\n",
        "        self.Wy -= learning_rate * dWy\n",
        "        self.by -= learning_rate * dby\n"
      ],
      "metadata": {
        "id": "kcFox-SMceKV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_lstm_shapes_and_grad(LSTM_class):\n",
        "    \"\"\"\n",
        "    Tests the student's LSTM class for:\n",
        "      A) shape correctness in forward pass\n",
        "      B) optional numerical gradient check for parameters\n",
        "    \"\"\"\n",
        "    print(\"========== Test LSTM layer ==========\")\n",
        "    np.random.seed(42)\n",
        "\n",
        "    # 1) Instantiate a small LSTM\n",
        "    time_steps = 3\n",
        "    input_dim = 4\n",
        "    hidden_dim = 5\n",
        "    output_dim = 2\n",
        "\n",
        "    model = LSTM_class(input_dim, hidden_dim, output_dim)\n",
        "\n",
        "    # 2) Create random input\n",
        "    X = np.random.randn(time_steps, input_dim)\n",
        "\n",
        "    # 3) Forward pass\n",
        "    y, h_final = model.forward(X)\n",
        "\n",
        "    # A) SHAPE CHECK\n",
        "    print(\"Forward output y shape:\", y.shape, f\"(expected {(output_dim, 1)})\")\n",
        "    print(\"Final hidden state shape:\", h_final.shape, f\"(expected {(hidden_dim, 1)})\")\n",
        "\n",
        "# Example usage:\n",
        "# from LSTM import LSTM\n",
        "test_lstm_shapes_and_grad(LSTM)"
      ],
      "metadata": {
        "id": "2H_g6YbKcFTZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the following, we:\n",
        "\n",
        "1. **Load and preprocess** the MNIST dataset from Keras, extracting 5,000 samples for training (`x_train_subset`, `y_train_subset`) and 1,000 for testing (`x_test_subset`, `y_test_subset`). We normalize pixel values to the [0, 1] range.\n",
        "\n",
        "2. **Define the `train_network` function**:\n",
        "   - Chooses whether to build an `RNN` or an `LSTM` model based on the `model_type` argument.\n",
        "   - Initializes tracking lists for losses and accuracies (`train_losses`, `test_losses`, etc.).\n",
        "   - Runs a loop over a fixed number of epochs (`epochs` = 10 by default).\n",
        "   - Within each epoch:\n",
        "     - Shuffles the training subset.\n",
        "     - Processes the data in mini-batches (`batch_size` = 64).\n",
        "     - For each sample in a batch:\n",
        "       1. **Forward pass** (`model.forward(...)`)  \n",
        "       2. **Compute loss** (`MSE` between outputs and one-hot targets)  \n",
        "       3. **Backward pass** (`model.backward(...)`) and updates parameters  \n",
        "       4. **Accumulate** train loss and count correct predictions\n",
        "     - Calculates average train loss and accuracy at the end of each epoch.\n",
        "     - **Tests** on the subset of 1,000 samples from MNIST, computing test loss and accuracy similarly (without parameter updates).\n",
        "   - Prints progress during training and summary results at the end.\n",
        "\n",
        "3. **At the bottom**:\n",
        "   - Trains and evaluates both an `RNN` and an `LSTM`, printing a final comparison of their test accuracies and training times.\n",
        "   - Optionally, calls plotting functions for performance curves and confusion matrices (if libraries are available).\n"
      ],
      "metadata": {
        "id": "cXq1H8anhvua"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================================\n",
        "# main\n",
        "# ========================================\n",
        "import numpy as np\n",
        "from keras.datasets import mnist\n",
        "import time\n",
        "\n",
        "# Load the MNIST dataset\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "print(f\"x_train shape: {x_train.shape}\")\n",
        "\n",
        "# Normalize pixel values to [0,1]\n",
        "x_train = x_train.astype(np.float32) / 255.0\n",
        "x_test  = x_test.astype(np.float32)  / 255.0\n",
        "\n",
        "# Hyperparameters\n",
        "input_dim    = 28\n",
        "hidden_dim   = 128\n",
        "output_dim   = 10\n",
        "\n",
        "# TODO 3: try to use different parameters\n",
        "learning_rate = 0.005\n",
        "batch_size    = 64\n",
        "epochs        = 9\n",
        "\n",
        "num_train_samples = 5000\n",
        "num_test_samples  = 1000\n",
        "\n",
        "x_train_subset = x_train[:num_train_samples]\n",
        "y_train_subset = y_train[:num_train_samples]\n",
        "x_test_subset  = x_test[:num_test_samples]\n",
        "y_test_subset  = y_test[:num_test_samples]\n",
        "\n",
        "def train_network(model_type='rnn'):\n",
        "    \"\"\"\n",
        "    Trains either an RNN or an LSTM on a subset of MNIST.\n",
        "    model_type: 'rnn' or 'lstm'\n",
        "    Returns: dictionary with model & metrics\n",
        "    \"\"\"\n",
        "\n",
        "    # from RNN import RNN\n",
        "    # from LSTM import LSTM\n",
        "    # Or assume they are already defined in the environment.\n",
        "\n",
        "    if model_type.lower() == 'rnn':\n",
        "        model = RNN(input_dim, hidden_dim, output_dim)\n",
        "        model_name = \"RNN\"\n",
        "    else:\n",
        "        model = LSTM(input_dim, hidden_dim, output_dim)\n",
        "        model_name = \"LSTM\"\n",
        "\n",
        "    train_losses = []\n",
        "    test_losses  = []\n",
        "    train_accuracies = []\n",
        "    test_accuracies  = []\n",
        "    start_time = time.time()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        epoch_loss    = 0.0\n",
        "        correct_train = 0\n",
        "        batch_count   = 0\n",
        "\n",
        "        # Shuffle the training data\n",
        "        indices     = np.random.permutation(len(x_train_subset))\n",
        "        x_shuffled  = x_train_subset[indices]\n",
        "        y_shuffled  = y_train_subset[indices]\n",
        "\n",
        "        # Go through batches\n",
        "        for i in range(0, len(x_shuffled), batch_size):\n",
        "            batch_count += 1\n",
        "            x_batch = x_shuffled[i:i+batch_size]\n",
        "            y_batch = y_shuffled[i:i+batch_size]\n",
        "\n",
        "            batch_loss = 0.0\n",
        "\n",
        "            # For each sample in the batch\n",
        "            for j in range(len(x_batch)):\n",
        "                # Forward pass\n",
        "                outputs, _ = model.forward(x_batch[j])\n",
        "\n",
        "                # Convert label y_batch[j] to one-hot\n",
        "                target = np.zeros((output_dim, 1))\n",
        "                target[y_batch[j]] = 1\n",
        "\n",
        "                # Compute MSE loss\n",
        "                sample_loss = np.mean((outputs - target)**2)\n",
        "                batch_loss += sample_loss\n",
        "\n",
        "                # Count accuracy\n",
        "                pred_class = np.argmax(outputs)\n",
        "                if pred_class == y_batch[j]:\n",
        "                    correct_train += 1\n",
        "\n",
        "                # Backprop\n",
        "                dLdy = 2 * (outputs - target)\n",
        "                model.backward(dLdy, learning_rate)\n",
        "\n",
        "            avg_batch_loss = batch_loss / len(x_batch)\n",
        "            epoch_loss += avg_batch_loss\n",
        "\n",
        "            # Print batch progress\n",
        "            if batch_count % 10 == 0:\n",
        "                print(f\"Epoch {epoch+1}/{epochs}, Batch {batch_count}, Loss: {avg_batch_loss:.4f}\")\n",
        "\n",
        "        # End of epoch: compute average train metrics\n",
        "        train_accuracy = correct_train / len(x_train_subset) * 100\n",
        "        train_accuracies.append(train_accuracy)\n",
        "\n",
        "        avg_epoch_loss = epoch_loss / (len(x_shuffled) // batch_size)\n",
        "        train_losses.append(avg_epoch_loss)\n",
        "\n",
        "        # Evaluate on test subset\n",
        "        test_loss    = 0.0\n",
        "        correct_test = 0\n",
        "\n",
        "        for i in range(len(x_test_subset)):\n",
        "            outputs, _ = model.forward(x_test_subset[i])\n",
        "            target = np.zeros((output_dim, 1))\n",
        "            target[y_test_subset[i]] = 1\n",
        "\n",
        "            sample_loss = np.mean((outputs - target)**2)\n",
        "            test_loss  += sample_loss\n",
        "\n",
        "            pred_class = np.argmax(outputs)\n",
        "            if pred_class == y_test_subset[i]:\n",
        "                correct_test += 1\n",
        "\n",
        "        avg_test_loss = test_loss / len(x_test_subset)\n",
        "        test_accuracy = correct_test / len(x_test_subset) * 100\n",
        "        test_losses.append(avg_test_loss)\n",
        "        test_accuracies.append(test_accuracy)\n",
        "\n",
        "        # Print epoch summary\n",
        "        print(f\"Epoch {epoch+1}/{epochs} | \"\n",
        "              f\"Train Loss: {avg_epoch_loss:.4f} | Train Acc: {train_accuracy:.2f}% | \"\n",
        "              f\"Test Loss: {avg_test_loss:.4f} | Test Acc: {test_accuracy:.2f}%\")\n",
        "\n",
        "    training_time = time.time() - start_time\n",
        "    print(f\"{model_name} Training completed in {training_time:.2f} seconds\")\n",
        "\n",
        "    return {\n",
        "        'model'           : model,\n",
        "        'train_losses'    : train_losses,\n",
        "        'test_losses'     : test_losses,\n",
        "        'train_accuracies': train_accuracies,\n",
        "        'test_accuracies' : test_accuracies,\n",
        "        'training_time'   : training_time\n",
        "    }\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Training RNN model...\")\n",
        "    rnn_results = train_network('rnn')\n",
        "\n",
        "    print(\"\\nTraining LSTM model...\")\n",
        "    lstm_results = train_network('lstm')\n",
        "\n",
        "    print(\"\\nComparison:\")\n",
        "    print(f\"RNN - Final test acc: {rnn_results['test_accuracies'][-1]:.2f}%, \"\n",
        "          f\"Training time: {rnn_results['training_time']:.2f}s\")\n",
        "\n",
        "    print(f\"LSTM - Final test acc: {lstm_results['test_accuracies'][-1]:.2f}%, \"\n",
        "          f\"Training time: {lstm_results['training_time']:.2f}s\")\n",
        "\n",
        "    try:\n",
        "        plot_performance_curves(rnn_results, lstm_results)\n",
        "        plot_confusion_matrices(rnn_results['model'], lstm_results['model'],\n",
        "                                x_test_subset, y_test_subset)\n",
        "    except ImportError:\n",
        "        print(\"Plotting libraries not available.\")\n"
      ],
      "metadata": {
        "id": "4ZZV8Gqcch5A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🎉 Success! 🎉\n",
        "\n",
        "If your **RNN's Test Accuracy** is above **80%** and your **LSTM's Test Accuracy** is above **90%**, you should:\n",
        "\n",
        "✅ Close your notebook.  \n",
        "✅ Save the result.  \n",
        "✅ Submit it.  \n",
        "✅ **Enjoy your day!** 😎  \n",
        "\n",
        "**Good job!** 🚀👏"
      ],
      "metadata": {
        "id": "uMGIMDRA197z"
      }
    }
  ]
}